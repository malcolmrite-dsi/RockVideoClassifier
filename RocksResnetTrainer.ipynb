{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RocksResnetTrainer",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malcolmrite-dsi/RockVideoClassifier/blob/main/RocksResnetTrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcruEbiWvZ43",
        "outputId": "43b6fcfb-724d-4bb5-c6a5-5981f1e2aee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSG4gi0zxw2C"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten, Dropout\n",
        "from tensorflow.keras.applications.resnet import preprocess_input\n",
        "from tensorflow.keras.applications import xception\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zaxYATlE01Q"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21-3DwUHHobs",
        "outputId": "10a1e04e-acb6-4e69-be93-df002febe774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(validation_split=0.2, preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/drive/My Drive/Module 2 shared folder/samples',\n",
        "        subset=\"training\",\n",
        "        seed=3,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=64,\n",
        "        class_mode='categorical')\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory( '/content/drive/My Drive/Module 2 shared folder/samples', \n",
        "        subset=\"validation\",\n",
        "        seed=3,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=64,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3132 images belonging to 5 classes.\n",
            "Found 781 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hksB_72TY-Ls",
        "outputId": "92b76314-a4bd-4458-9ed8-4e8346a90c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/drive/My Drive/Module 2 shared folder/samples',\n",
        "        subset=\"training\",\n",
        "        seed=3,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=64,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3913 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xATOlSeOUaz",
        "outputId": "e32ab35f-6e92-4f5e-c89e-242227225e7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "resnet = keras.applications.ResNet50(include_top=False, pooling=\"max\", input_shape=(64,64,3))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DothacpCQIN6"
      },
      "source": [
        "# mark loaded layers as not trainable\n",
        "for layer in resnet.layers:\n",
        "  layer.trainable = False \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pm0ce-m0PgR"
      },
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  #keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIDhw6JqQNCn"
      },
      "source": [
        "# mark loaded layers as not trainable\n",
        "#for layer in resnet.layers:\n",
        "\t#layer.trainable = False \n",
        "\n",
        "flat = Flatten()(resnet.layers[-1].output)\n",
        "dense = Dense(1024, activation='relu')(flat)\n",
        "output = Dense(5, activation='softmax')(dense)\n",
        "model = Model(inputs=resnet.inputs, outputs=output)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV4cK0F_y7uj"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWcigw5IJQcL",
        "outputId": "3439a3c2-b1c0-49cb-8d5d-9079b9fa2b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=[\"categorical_accuracy\"])\n",
        "checkpoint_best = keras.callbacks.ModelCheckpoint(\"/content/drive/My Drive/model_best.h5\", \n",
        "            monitor='loss', verbose=0, save_best_only=True, save_weights_only=False, save_freq='epoch')\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(\"/content/drive/My Drive/model_last.h5\", \n",
        "            verbose=0, save_best_only=False, save_weights_only=False, save_freq='epoch')\n",
        "model.fit(\n",
        "        train_generator,\n",
        "        epochs = 5,\n",
        "        validation_data=val_generator,\n",
        "        callbacks=[checkpoint_best]\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "49/49 [==============================] - 921s 19s/step - loss: 1.4298 - categorical_accuracy: 0.7925 - val_loss: 0.6334 - val_categorical_accuracy: 0.8656\n",
            "Epoch 2/5\n",
            "49/49 [==============================] - 63s 1s/step - loss: 0.2659 - categorical_accuracy: 0.9119 - val_loss: 0.4664 - val_categorical_accuracy: 0.8707\n",
            "Epoch 3/5\n",
            "49/49 [==============================] - 61s 1s/step - loss: 0.1628 - categorical_accuracy: 0.9406 - val_loss: 0.4552 - val_categorical_accuracy: 0.8681\n",
            "Epoch 4/5\n",
            "49/49 [==============================] - 62s 1s/step - loss: 0.1231 - categorical_accuracy: 0.9527 - val_loss: 0.4680 - val_categorical_accuracy: 0.8694\n",
            "Epoch 5/5\n",
            "49/49 [==============================] - 63s 1s/step - loss: 0.1004 - categorical_accuracy: 0.9623 - val_loss: 0.5946 - val_categorical_accuracy: 0.8438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f08a3929f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGSNH0THX8sV",
        "outputId": "6cd9f89e-2abc-4918-f786-dbdd031f6835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model.evaluate(val_generator)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 36s 3s/step - loss: 0.4126 - categorical_accuracy: 0.8822\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4126453995704651, 0.8822023272514343]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghn0JOxbjcyh",
        "outputId": "cdea0418-4f2f-407c-ebe4-5786b667fcfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "model.fit(\n",
        "        train_generator,\n",
        "        initial_epoch=10,\n",
        "        epochs = 25,\n",
        "        validation_data=val_generator, callbacks=[checkpoint, checkpoint_best]\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/25\n",
            "62/62 [==============================] - 72s 1s/step - loss: 0.1822 - categorical_accuracy: 0.9443 - val_loss: 0.1983 - val_categorical_accuracy: 0.9283\n",
            "Epoch 12/25\n",
            "62/62 [==============================] - 74s 1s/step - loss: 0.1454 - categorical_accuracy: 0.9535 - val_loss: 0.2430 - val_categorical_accuracy: 0.9283\n",
            "Epoch 13/25\n",
            "62/62 [==============================] - 76s 1s/step - loss: 0.1018 - categorical_accuracy: 0.9660 - val_loss: 0.0794 - val_categorical_accuracy: 0.9718\n",
            "Epoch 14/25\n",
            "62/62 [==============================] - 79s 1s/step - loss: 0.0633 - categorical_accuracy: 0.9785 - val_loss: 0.0779 - val_categorical_accuracy: 0.9731\n",
            "Epoch 15/25\n",
            "62/62 [==============================] - 74s 1s/step - loss: 0.0874 - categorical_accuracy: 0.9737 - val_loss: 0.1695 - val_categorical_accuracy: 0.9488\n",
            "Epoch 16/25\n",
            "62/62 [==============================] - 78s 1s/step - loss: 0.0601 - categorical_accuracy: 0.9806 - val_loss: 0.0240 - val_categorical_accuracy: 0.9949\n",
            "Epoch 17/25\n",
            "62/62 [==============================] - 81s 1s/step - loss: 0.0310 - categorical_accuracy: 0.9921 - val_loss: 0.0234 - val_categorical_accuracy: 0.9936\n",
            "Epoch 18/25\n",
            "62/62 [==============================] - 76s 1s/step - loss: 0.0205 - categorical_accuracy: 0.9954 - val_loss: 0.0180 - val_categorical_accuracy: 0.9923\n",
            "Epoch 19/25\n",
            "62/62 [==============================] - 75s 1s/step - loss: 0.0172 - categorical_accuracy: 0.9967 - val_loss: 0.0146 - val_categorical_accuracy: 0.9987\n",
            "Epoch 20/25\n",
            "62/62 [==============================] - 74s 1s/step - loss: 0.0191 - categorical_accuracy: 0.9977 - val_loss: 0.0091 - val_categorical_accuracy: 0.9987\n",
            "Epoch 21/25\n",
            "62/62 [==============================] - 81s 1s/step - loss: 0.0145 - categorical_accuracy: 0.9980 - val_loss: 0.0104 - val_categorical_accuracy: 0.9962\n",
            "Epoch 22/25\n",
            "62/62 [==============================] - 77s 1s/step - loss: 0.0113 - categorical_accuracy: 0.9982 - val_loss: 0.0028 - val_categorical_accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "62/62 [==============================] - 76s 1s/step - loss: 0.0099 - categorical_accuracy: 0.9987 - val_loss: 0.0059 - val_categorical_accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "62/62 [==============================] - 76s 1s/step - loss: 0.0104 - categorical_accuracy: 0.9982 - val_loss: 0.0067 - val_categorical_accuracy: 0.9987\n",
            "Epoch 25/25\n",
            "62/62 [==============================] - 76s 1s/step - loss: 0.0174 - categorical_accuracy: 0.9972 - val_loss: 0.0038 - val_categorical_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f089905bf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YCK1lbGTRr4"
      },
      "source": [
        "model.save(\"/content/drive/My Drive/model_best.h5\")"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}